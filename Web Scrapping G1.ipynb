{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544fcbc5-0186-486d-a122-1158f71aa75a",
   "metadata": {},
   "source": [
    "# Relatório sobre o Código de Raspagem de Notícias do G1\n",
    "\n",
    "## Objetivo\n",
    "O código tem como objetivo coletar notícias do site G1 relacionadas a eventos climáticos (chuvas, ciclones, tempestades, etc.) usando Selenium e BeautifulSoup. Ele extrai os seguintes campos de cada notícia:\n",
    "- **Produto**\n",
    "- **Link**\n",
    "- **Título**\n",
    "- **Data**\n",
    "- **Conteúdo**\n",
    "\n",
    "## Funcionalidade\n",
    "\n",
    "### 1. **Bibliotecas Importadas**\n",
    "- **BeautifulSoup**: Para parsear o HTML e extrair dados.\n",
    "- **Selenium**: Para interagir com a página dinâmica.\n",
    "- **pandas**: Para estruturar e salvar os dados.\n",
    "- **tqdm**: Para exibir barra de progresso.\n",
    "- **time**: Para medir o tempo de execução.\n",
    "- **urllib.parse**: Para tratar URLs.\n",
    "\n",
    "### 2. **Função Principal: `scrape_infinite_scroll`**\n",
    "- **Entrada**: URL da pesquisa e número de itens a serem coletados.\n",
    "- **Processo**:\n",
    "  - Navega pelas páginas do G1 utilizando Selenium.\n",
    "  - Coleta os blocos de notícias com BeautifulSoup.\n",
    "  - Para cada bloco, extrai:\n",
    "    - **Produto**: Categoria da notícia.\n",
    "    - **Link**: URL da página completa.\n",
    "    - **Título**: Título da notícia.\n",
    "    - **Data**: Data da publicação.\n",
    "    - **Conteúdo**: Descrição resumida.\n",
    "  - Repete o processo até coletar o número especificado de notícias.\n",
    "\n",
    "### 3. **Configuração do Selenium**\n",
    "- Configurações para rodar em modo \"headless\" (sem interface gráfica).\n",
    "- Configuração de \"user-agent\" para simular navegação real.\n",
    "- Navegação nas páginas do G1 com a URL dinâmica (para carregar mais notícias).\n",
    "\n",
    "### 4. **Execução do Processo**\n",
    "- O usuário define o número de notícias a coletar e o intervalo de anos.\n",
    "- O código gera a URL de pesquisa para cada ano e coleta as notícias.\n",
    "- Os dados são armazenados em um DataFrame `df` e exportados para um arquivo Excel.\n",
    "\n",
    "### 5. **Controle de Tempo**\n",
    "- O tempo de execução da raspagem é calculado e exibido.\n",
    "\n",
    "## Campos Coletados\n",
    "- **Produto**: Categoria da notícia (ex: \"Previsão do Tempo\").\n",
    "- **Link**: URL da notícia.\n",
    "- **Título**: Título da notícia.\n",
    "- **Data**: Data de publicação.\n",
    "- **Conteúdo**: Resumo da notícia.\n",
    "\n",
    "## Fluxo de Execução\n",
    "1. O script solicita ao usuário o número de notícias e o intervalo de anos.\n",
    "2. Realiza buscas no G1 com palavras-chave (ex: \"Chuva, Ciclone\").\n",
    "3. Navega pelas páginas de resultados e coleta os dados.\n",
    "4. Armazena e exporta os dados para um arquivo Excel.\n",
    "\n",
    "## Pontos de Melhoria\n",
    "- **Tratamento de Exceções**: Melhorar a captura de erros específicos para tornar a raspagem mais robusta.\n",
    "- **Otimização**: Evitar carregamento desnecessário de páginas.\n",
    "- **Validação de Dados**: Verificar se os campos coletados estão completos antes de salvar.\n",
    "- **Uso de Proxies/Delay**: Para evitar bloqueios no site G1.\n",
    "\n",
    "## Conclusão\n",
    "O código realiza a raspagem de notícias de forma eficiente, coletando informações sobre eventos climáticos do site G1. Ele organiza os dados em um DataFrame e exporta para um arquivo Excel, facilitando a análise posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f3892-24dd-49ba-a096-5e09c9c0989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def scrape_infinite_scroll(url, num_items):\n",
    "    start_time = time.time()  \n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    scraped_data = []\n",
    "    noticia = {\"\"}\n",
    "\n",
    "    page_counter = 1\n",
    "\n",
    "    with tqdm(total=num_items, desc=\"Raspando dados\") as pbar:\n",
    "        \n",
    "        while len(scraped_data) < num_items:\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            product_blocks = soup.find_all('li', {'class': 'widget widget--card widget--info'})\n",
    "            \n",
    "            for block in product_blocks:    \n",
    "                if not block.find('a').get('href'):\n",
    "                    continue\n",
    "                try:\n",
    "                    if len(scraped_data) >= num_items:\n",
    "                        pbar.update(1)\n",
    "                        break\n",
    "                    noticia = {\"Produto\": block.find('div', class_='widget--info__header').text.strip(),\n",
    "                               \"Link\": unquote(\"https\" + block.find('a').get('href').split(\"https\")[1]).split(\"&syn\")[0],\n",
    "                               \"Título\": block.find('div', 'widget--info__title').text.strip(),\n",
    "                               \"Data\": block.find('div', 'widget--info__meta').text.strip(),\n",
    "                               \"Conteúdo\": block.find('p', class_='widget--info__description').text.strip()\n",
    "                              }\n",
    "                    \n",
    "                    scraped_data.append(noticia)\n",
    "                except:\n",
    "                    pass\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "            page_counter += 1\n",
    "            driver.get(url + f\"&page={page_counter}\")\n",
    "\n",
    "\n",
    "    end_time = time.time()  # Captura o tempo de término\n",
    "    total_time = end_time - start_time  # Calcula o tempo total de execução\n",
    "    \n",
    "    print(f\"Tempo total de execução: {total_time:.2f} segundos\")\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    num_items = int(input('Digite o número de notícias que deseja coletar por ano: '))\n",
    "    \n",
    "    palavra_chave = \"Chuva, Temporal, Tempestade, Ciclone, Previsão, Frente fria\"\n",
    "    \n",
    "    ano_inicial = int(input(\"Ano inicial da pesquisa: \"))\n",
    "    ano_final = int(input(\"Ano final da pesquisa: \"))\n",
    "\n",
    "    arquivo = input(\"Arquivo para continuar\")\n",
    "\n",
    "    print(\"Iniciando selenium...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--disable-web-security')\n",
    "    options.add_argument('--disable-site-isolation-trials')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--allow-running-insecure-content')\n",
    "    options.add_argument('--disable-notifications')\n",
    "\n",
    "    print(\"Configurando user agent...\")\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    \n",
    "    print(\"Iniciando webdriver...\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "        \n",
    "    print(\"Inicializando dataFrame...\")\n",
    "    if arquivo != \"\":\n",
    "        df = pd.read_excel(arquivo)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"Produto\", \"Link\", \"Título\",\"Data\",\"Conteúdo\"])\n",
    "    \n",
    "    scraped_data = []\n",
    "    for ano in range(ano_inicial, ano_final+1):\n",
    "        url = f\"https://g1.globo.com/busca/?q={palavra_chave}&order=recent&from={ano}-01-01T00%3A00%3A00-0200&to={ano}-12-30T23%3A59%3A59-0200\"  # URL com palavra-chave\n",
    "        print(f\"\\nBuscando {ano}\")\n",
    "        scraped_data = scrape_infinite_scroll(url, num_items)\n",
    "\n",
    "        dfTemp = pd.DataFrame(list(map(lambda x: x.values(), scraped_data)) ,columns=scraped_data[0].keys())\n",
    "        df = pd.concat([df, dfTemp])\n",
    "        \n",
    "        df.to_excel('./planilhas/Noticias_cruas_duplicatas.xlsx', index=False)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    # Convertendo os dados para um DataFrame do pandas\n",
    "    \n",
    "    # Salvando o DataFrame em um arquivo Excel\n",
    "    print(\"Raspagem de dados finalizada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837418bf-61df-4276-93a2-3084c9acabea",
   "metadata": {},
   "source": [
    "Esse código a baixo é uma variação do primeiro para realizar testes no ano de 2020 com cada palavra chave sendo buscada individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75657c24-e8ce-4261-b4d1-655885bb696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def scrape_infinite_scroll(url, num_items):\n",
    "    start_time = time.time()  \n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    scraped_data = []\n",
    "    noticia = {\"\"}\n",
    "\n",
    "    page_counter = 1\n",
    "\n",
    "    with tqdm(total=num_items, desc=\"Raspando dados\") as pbar:\n",
    "        \n",
    "        while len(scraped_data) < num_items:\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            product_blocks = soup.find_all('li', {'class': 'widget widget--card widget--info'})\n",
    "            \n",
    "            for block in product_blocks:    \n",
    "                if not block.find('a').get('href'):\n",
    "                    continue\n",
    "                try:\n",
    "                    if len(scraped_data) >= num_items:\n",
    "                        pbar.update(1)\n",
    "                        break\n",
    "                    noticia = {\"Produto\": block.find('div', class_='widget--info__header').text.strip(),\n",
    "                               \"Link\": unquote(\"https\" + block.find('a').get('href').split(\"https\")[1]).split(\"&syn\")[0],\n",
    "                               \"Título\": block.find('div', 'widget--info__title').text.strip(),\n",
    "                               \"Data\": block.find('div', 'widget--info__meta').text.strip(),\n",
    "                               \"Conteúdo\": block.find('p', class_='widget--info__description').text.strip()\n",
    "                              }\n",
    "                    \n",
    "                    scraped_data.append(noticia)\n",
    "                except:\n",
    "                    pass\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "            # Avança para a próxima página\n",
    "            page_counter += 1\n",
    "            driver.get(url + f\"&page={page_counter}\")\n",
    "\n",
    "\n",
    "    end_time = time.time()  # Captura o tempo de término\n",
    "    total_time = end_time - start_time  # Calcula o tempo total de execução\n",
    "    \n",
    "    print(f\"Tempo total de execução: {total_time:.2f} segundos\")\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    num_items = int(input('Digite o número de notícias que deseja coletar por ano: '))\n",
    "    \n",
    "    palavras_chave = [\"Chuva\", \"Temporal\", \"Tempestade\", \"Ciclone\", \"Previsão\", \"Frente fria\"]\n",
    "    \n",
    "    ano_inicial = int(input(\"Ano inicial da pesquisa: \"))\n",
    "    ano_final = int(input(\"Ano final da pesquisa: \"))\n",
    "\n",
    "    arquivo = input(\"Arquivo para continuar\")\n",
    "\n",
    "    print(\"Iniciando selenium...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--disable-web-security')\n",
    "    options.add_argument('--disable-site-isolation-trials')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--allow-running-insecure-content')\n",
    "    options.add_argument('--disable-notifications')\n",
    "\n",
    "    print(\"Configurando user agent...\")\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    \n",
    "    print(\"Iniciando webdriver...\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "        \n",
    "    print(\"Inicializando dataFrame...\")\n",
    "    if arquivo != \"\":\n",
    "        df = pd.read_excel(arquivo)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"Produto\", \"Link\", \"Título\",\"Data\",\"Conteúdo\"])\n",
    "    \n",
    "    scraped_data = []\n",
    "    for chave in palavras_chave:\n",
    "        url = f\"https://g1.globo.com/busca/?q={chave}&order=recent&from={2020}-01-01T00%3A00%3A00-0200&to={2020}-12-30T23%3A59%3A59-0200\"  # URL com palavra-chave\n",
    "        print(f\"\\nBuscando {chave}\")\n",
    "        scraped_data = scrape_infinite_scroll(url, num_items)\n",
    "\n",
    "        dfTemp = pd.DataFrame(list(map(lambda x: x.values(), scraped_data)) ,columns=scraped_data[0].keys())\n",
    "        df = pd.concat([df, dfTemp])\n",
    "        \n",
    "        df.to_excel('./planilhas/Noticias_cruas_duplicatas.xlsx', index=False)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    # Convertendo os dados para um DataFrame do pandas\n",
    "    \n",
    "    # Salvando o DataFrame em um arquivo Excel\n",
    "    print(\"Raspagem de dados finalizada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6d724-ee86-41ee-b1a7-af5ea0425eb9",
   "metadata": {},
   "source": [
    "## Este código remove as notícias duplicadas de uma planilha, considerando o título como critério para identificação de duplicidade. Ele carrega os dados de um arquivo Excel, calcula o número de títulos únicos, remove as duplicatas e salva o resultado em um novo arquivo Excel. A quantidade final de registros após a remoção das duplicatas é exibida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa914fc-ee16-409c-9908-8f372a1121a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"./planilhas/Noticias_cruas_duplicatas.xlsx\")\n",
    "\n",
    "print(len(list(set(df[\"Título\"]))))\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "noticias_filtradas = df.drop_duplicates()\n",
    "\n",
    "print(f'Ficaram {len(noticias_filtradas)} resgistros de notícias após a retirada das notícia duplicicadas.')\n",
    "\n",
    "noticias_filtradas.to_excel(\"./planilhas/Noticias_sem_duplicadas.xlsx\", index=False)\n",
    "display(noticias_filtradas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ca7f5-988a-48a2-bc03-88cc73182aef",
   "metadata": {},
   "source": [
    "### Mini Relatório: Análise do Código\n",
    "\n",
    "**Objetivo:** Exibir a quantidade de notícias encontradas por ano (2006–2024) a partir de datas em um arquivo Excel.  \n",
    "\n",
    "**Funcionamento:**\n",
    "1. **Importação:** Usa a biblioteca `pandas` para manipular dados da planilha.\n",
    "2. **Leitura:** Lê a planilha `Noticias_sem_duplicadas.xlsx` e converte a coluna \"Data\" em uma lista.\n",
    "3. **Criação do Dicionário:** Inicializa um dicionário com anos (2006–2024) como chaves e contagens iniciadas em 0.\n",
    "4. **Extração de Anos:** Usa `lambda` para capturar os 4 caracteres do ano de cada data no formato \"DD/MM/AAAA\".\n",
    "5. **Contagem:** Incrementa a contagem de notícias no ano correspondente, ignorando datas malformadas.\n",
    "6. **Saída:** Exibe o dicionário com a contagem de notícias por ano. \n",
    "\n",
    "**Exemplo de saída:**  \n",
    "```python\n",
    "{'2024': 5, '2023': 12, ..., '2006': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1edb5-1b4e-4937-8ef3-5559611a792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp = pd.read_excel(\"./planilhas/Noticias_sem_duplicadas.xlsx\")\n",
    "temp = list(temp[\"Data\"])\n",
    "a = {str(x):0 for x in list(range(2006,2025))[::-1]}\n",
    "for x in list(map(lambda x: x[6:10], temp)):\n",
    "    try:\n",
    "        a[x] += 1\n",
    "    except:\n",
    "        pass\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f1564-ce40-4f0b-be16-7a2d6f46de4c",
   "metadata": {},
   "source": [
    "### Mini Relatório: Análise do Código\n",
    "\n",
    "**Objetivo:** Identificar e associar notícias a cidades e/ou regiões de Santa Catarina (SC) através de análise textual no título e conteúdo das notícias.\n",
    "\n",
    "---\n",
    "\n",
    "**Funcionamento:**  \n",
    "1. **Importação e Leitura:**  \n",
    "   - Lê as listas de cidades (arquivo `cidade_sc1.xlsx`) e notícias (`Noticias_sem_duplicadas.xlsx`).\n",
    "\n",
    "2. **Busca Textual:**  \n",
    "   - Verifica, para cada notícia, se cidades ou regiões aparecem no título ou conteúdo.  \n",
    "   - Filtra entradas com dados inválidos.\n",
    "\n",
    "3. **Atribuição de Região:**  \n",
    "   - Associa a cidade encontrada a uma região correspondente se nenhuma região foi explicitamente mencionada.\n",
    "\n",
    "4. **Atualização:**  \n",
    "   - Adiciona colunas \"Cidade\" e \"Região\" ao DataFrame de notícias.\n",
    "\n",
    "5. **Exportação:**  \n",
    "   - Salva o DataFrame atualizado no arquivo `Notícias_cidade_regiao_definida.xlsx`.\n",
    "\n",
    "**Saída:**  \n",
    "Notícias classificadas por cidade e região com dados consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e18b7-9706-45a0-b5c9-3281af6bcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cidades_sc1 = pd.read_excel('./planilhas/cidade_sc1.xlsx')\n",
    "noticias_filtradas = pd.read_excel(\"./planilhas/Noticias_sem_duplicadas.xlsx\")\n",
    "\n",
    "# Loop sobre as notícias\n",
    "regioes = ['SUL', 'VALE DO ITAJAÍ', 'NORTE', 'GRANDE FLORIANÓPOLIS', 'SERRANA', 'OESTE']\n",
    "for index, noticia in noticias_filtradas.iterrows():\n",
    "    cidade_encontrada = []  # Valor padrão caso nenhuma cidade seja encontrada na notícia\n",
    "    regiao_encontrada = []\n",
    "    # Loop sobre as cidades de Santa Catarina\n",
    "    if type(noticia[\"Título\"]) != str or type(noticia[\"Conteúdo\"]) != str:\n",
    "            continue\n",
    "        \n",
    "    for cidade in cidades_sc1['MUNICIPIO']:\n",
    "        if cidade.strip().upper() in noticia['Título'].upper() or cidade.strip().upper() in noticia[\"Conteúdo\"].upper():  \n",
    "            cidade_encontrada.append(cidade.strip()) \n",
    "\n",
    "    for regiao in regioes:\n",
    "        if regiao.strip().upper() in noticia[\"Título\"].upper() or regiao.strip().upper() in noticia[\"Conteúdo\"].upper():\n",
    "            regiao_encontrada.append(regiao)\n",
    "\n",
    "\n",
    "    if cidade_encontrada and not regiao_encontrada:\n",
    "        for cidade in cidade_encontrada:\n",
    "            idx = list(map(lambda x: x[0],cidades_sc1.values)).index(cidade) \n",
    "            regiao_encontrada.append(cidades_sc1.values[idx][1])\n",
    "            \n",
    "    noticias_filtradas.at[index, 'Cidade'] = str(list(set(cidade_encontrada)))\n",
    "    noticias_filtradas.at[index, \"Região\"] = str(list(set(regiao_encontrada)))\n",
    "\n",
    "noticias_filtradas.to_excel('./planilhas/Notícias_cidade_regiao_definida.xlsx', index=False)\n",
    "\n",
    "len(noticias_filtradas)\n",
    "\n",
    "#display(noticias_filtradas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb83d54-95db-4459-b9c7-9f0288b7c25a",
   "metadata": {},
   "source": [
    "### Mini Relatório: Análise do Código\n",
    "\n",
    "**Objetivo:**  \n",
    "Filtrar as notícias relacionadas a Santa Catarina (SC) utilizando palavras-chave e nomes de cidades do estado. Esse filtro complementa o anterior e avalia a eficácia de diferentes abordagens.\n",
    "\n",
    "---\n",
    "\n",
    "**Funcionamento:**  \n",
    "\n",
    "1. **Importação e Preparação:**  \n",
    "   - Lê os arquivos:\n",
    "     - **Notícias:** `Noticias_sem_duplicadas.xlsx`.  \n",
    "     - **Cidades SC:** `cidade_sc1.xlsx`.  \n",
    "   - Inicializa a lista `a` com valores `False` para marcar notícias que serão mantidas.\n",
    "\n",
    "2. **Filtragem no Título:**  \n",
    "   - Verifica se os títulos das notícias contêm:\n",
    "     - Palavras-chave: `\"santa catarina\"`, `\"santa-catarina\"`, ou `\" sc \"`.  \n",
    "     - Nomes de cidades da lista.  \n",
    "   - Atualiza a lista `a` para `True` caso qualquer condição seja atendida.\n",
    "\n",
    "3. **Filtragem no Conteúdo:**  \n",
    "   - Similar ao título, mas aplicado ao texto do conteúdo, ignorando valores não textuais.\n",
    "\n",
    "4. **Aplicação do Filtro:**  \n",
    "   - Retém no DataFrame apenas as notícias marcadas como relevantes (`True`).\n",
    "\n",
    "5. **Exportação:**  \n",
    "   - Salva o DataFrame filtrado no arquivo `Filtragem_palavra_individual_cidades.xlsx`.\n",
    "\n",
    "---\n",
    "\n",
    "**Saída:**  \n",
    "Um novo arquivo contendo apenas as notícias filtradas relacionadas a SC com base em palavras-chave e nomes de cidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cdd48-ec49-4b89-928f-8fac287e6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "noticias_filtradas = pd.read_excel(\"./planilhas/Noticias_sem_duplicadas.xlsx\")\n",
    "cidades_sc1 = pd.read_excel('./planilhas/cidade_sc1.xlsx')\n",
    "print(len(noticias_filtradas))\n",
    "\n",
    "\n",
    "a = [False] * len(noticias_filtradas)\n",
    "\n",
    "for i, noticia in enumerate(noticias_filtradas[\"Título\"]):\n",
    "    if \"santa catarina\" in noticia.lower() or \"santa-catarina\" in noticia.lower() or \" sc \" in noticia.lower():\n",
    "        a[i] = True\n",
    "    for cidade in cidades_sc1[\"MUNICIPIO\"]:\n",
    "        if cidade.strip().lower() in noticia.lower():\n",
    "            a[i] = True\n",
    "            break\n",
    "            \n",
    "for i, noticia in enumerate(noticias_filtradas[\"Conteúdo\"]):\n",
    "    if type(noticia) != str:\n",
    "        continue\n",
    "    if \"santa catarina\" in noticia.lower() or \"santa-catarina\" in noticia.lower() or \" sc \" in noticia.lower():\n",
    "        a[i] = True\n",
    "    for cidade in cidades_sc1[\"MUNICIPIO\"]:\n",
    "        if cidade.strip().lower() in noticia.lower():\n",
    "            a[i] = True\n",
    "            break\n",
    "\n",
    "noticias_filtradas = noticias_filtradas[a]  \n",
    "\n",
    "\n",
    "\n",
    "# noticias_filtradas.to_excel('Notícias_cidade_regiao_definida.xlsx', index=False)\n",
    "noticias_filtradas.to_excel('./planilhas/Filtragem_palavra_individual_cidades.xlsx', index=False)\n",
    "\n",
    "len(noticias_filtradas)\n",
    "\n",
    "#display(noticias_filtradas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d178ac5-c7bb-4acd-82d8-13e968d6e255",
   "metadata": {},
   "source": [
    "### Mini Relatório: Análise do Código\n",
    "\n",
    "**Objetivo:**  \n",
    "Filtrar as notícias que mencionam Santa Catarina e excluir as que fazem referência a outros estados brasileiros, usando palavras-chave, links e informações regionais.\n",
    "\n",
    "---\n",
    "\n",
    "**Funcionamento:**  \n",
    "\n",
    "1. **Importação e Leitura de Dados:**  \n",
    "   - Lê o arquivo `Notícias_cidade_regiao_definida.xlsx`, contendo notícias pré-filtradas, e seleciona aquelas com links válidos.  \n",
    "\n",
    "2. **Primeiro Filtro:**  \n",
    "   - Retém notícias que:\n",
    "     - Contêm \"santa-catarina\", \"/sc/\", ou links associados à SC.\n",
    "     - Possuem cidades definidas ou pertencem ao produto \"G1\".\n",
    "\n",
    "3. **Checagem de Estados:**  \n",
    "   - Analisa o texto das notícias e links buscando nomes ou siglas de estados diferentes de SC.\n",
    "   - Caso outro estado seja encontrado, a notícia é marcada como irrelevante.\n",
    "\n",
    "4. **Cálculo e Exportação:**  \n",
    "   - Calcula o número e percentual de notícias relevantes filtradas.\n",
    "   - Salva o resultado no arquivo `Filtradas link SC.xlsx`.\n",
    "\n",
    "---\n",
    "\n",
    "**Saída:**  \n",
    "- **Total de notícias originais:** Número de entradas antes do filtro.  \n",
    "- **Notícias relevantes:** Quantidade e percentual de notícias relacionadas a SC após o filtro.  \n",
    "- **Exportação:** Arquivo Excel com as notícias relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e99a8d-66ff-4683-9f5c-6ce00d88456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "\n",
    "noticias_filtradas = pd.read_excel(\"./planilhas/Notícias_cidade_regiao_definida.xlsx\")\n",
    "\n",
    "filtered_df = noticias_filtradas[list(map(lambda x: \"https\" in x , noticias_filtradas[\"Link\"]))]\n",
    "\n",
    "\n",
    "RegioesBrasil = ['DF', 'Distrito Federal', 'GO', 'Goiás', 'MT', 'Mato Grosso', 'MS', 'Mato Grosso do Sul', 'Região Nordeste', 'AL', 'Alagoas', 'BA', 'Bahia', 'CE', 'Ceará', 'MA', 'Maranhão', 'PB', 'Paraíba', 'PE', 'Pernambuco', 'PI', 'Piauí', 'RN', 'Rio Grande do Norte', 'SE', 'Sergipe', 'Região Norte', 'AC', 'Acre', 'AP', 'Amapá', 'AM', 'Amazonas', 'PA', 'Pará', 'RO', 'Rondônia', 'RR', 'Roraima', 'TO', 'Tocantis', 'Região Sudeste', 'ES', 'Espírito Santo', 'MG', 'Minas Gerais', 'RJ', 'Rio de Janeiro', 'SP', 'São Paulo', 'PR', 'Paraná', 'RS', 'Rio Grande do Sul']\n",
    "\n",
    "filtered_df = filtered_df[list(map(lambda x, y, z: z == \"G1\" or y != \"[]\" or \"santa-catarina\" in x.split(\"https\")[1] or \"/sc/\" in x, filtered_df[\"Link\"], filtered_df[\"Cidade\"], filtered_df[\"Produto\"]))]\n",
    "print(len(filtered_df))\n",
    "outraRegiao = [True]*len(filtered_df)\n",
    "\n",
    "for noticia in range(len(filtered_df)):\n",
    "    text = str(filtered_df.values[noticia][2]) + str(filtered_df.values[noticia][4])\n",
    "    link = filtered_df[\"Link\"][noticia]\n",
    "\n",
    "    for x in RegioesBrasil:\n",
    "        if x in text:\n",
    "            outraRegiao[noticia] = False\n",
    "            break\n",
    "        if len(x) == 2:\n",
    "            if \"F\" + x.lower() + \"%\" in link:\n",
    "                outraRegiao[noticia] = False\n",
    "                print(link)\n",
    "        elif x.lower() in link or x.lower().replace(\" \", \"-\") in link:\n",
    "                outraRegiao[noticia] = False\n",
    "\n",
    "# if filtered_df.values[noticia][]\n",
    "filtered_df = filtered_df[outraRegiao]\n",
    "\n",
    "total_noticias = len(noticias_filtradas)\n",
    "\n",
    "num_noticias_filtradas = len(filtered_df)\n",
    "\n",
    "percentual_filtradas = (num_noticias_filtradas / total_noticias) * 100\n",
    "\n",
    "noticias_filtradas = filtered_df\n",
    "noticias_filtradas.to_excel('./planilhas/Filtradas link SC.xlsx', index=False)\n",
    "\n",
    "print(f\"Total de notícias: {total_noticias}\")\n",
    "print(f\"Número de notícias 'G1' ou 'G1 SC': {num_noticias_filtradas}\")\n",
    "print(f\"Percentual correspondente: {percentual_filtradas:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a61620-e275-4ea1-99ac-25fadb259667",
   "metadata": {},
   "source": [
    "### Mini Relatório: Análise do Código\n",
    "\n",
    "**Objetivo:**  \n",
    "Realizar a última filtragem de notícias antes do acesso individual às páginas. Este filtro prioriza links do **G1** e verifica a presença de \"Santa Catarina\" nos links, mas ressalta as inconsistências do padrão de formatação ao longo dos anos.\n",
    "\n",
    "---\n",
    "\n",
    "**Funcionamento:**  \n",
    "\n",
    "1. **Importação e Leitura de Dados:**  \n",
    "   - Lê o arquivo `Notícias_cidade_regiao_definida.xlsx`, contendo notícias filtradas anteriormente.  \n",
    "\n",
    "2. **Primeiro Filtro:**  \n",
    "   - Seleciona apenas notícias com links válidos contendo \"https\".\n",
    "   - Retém notícias baseadas em critérios:\n",
    "     - Publicadas pelo **G1**.\n",
    "     - Contêm informações de cidades definidas.\n",
    "     - Incluem \"santa-catarina\" ou \"/sc/\" no link.\n",
    "\n",
    "3. **Eliminação de Outras Regiões:**  \n",
    "   - Para cada notícia, analisa o texto e link em busca de estados/regiões fora de SC.\n",
    "   - Notícias com referências a outros estados são descartadas.\n",
    "\n",
    "4. **Cálculo Estatístico:**  \n",
    "   - Calcula o número total de notícias e o percentual das filtradas que ainda são relevantes.\n",
    "\n",
    "5. **Exportação:**  \n",
    "   - Salva o resultado no arquivo `Filtradas link SC.xlsx` para o próximo passo do processo.\n",
    "\n",
    "---\n",
    "\n",
    "**Saída:**  \n",
    "- **Total de notícias:** Quantidade inicial antes do filtro.  \n",
    "- **Notícias relevantes:** Quantidade e percentual de notícias filtradas que continuam relevantes.  \n",
    "- **Exportação:** Arquivo final das notícias aptas para o acesso individual.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413e25d-0a09-49ca-bb8f-73deac6c06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "\n",
    "noticias_filtradas = pd.read_excel(\"./planilhas/Notícias_cidade_regiao_definida.xlsx\")\n",
    "\n",
    "filtered_df = noticias_filtradas[list(map(lambda x: \"https\" in x , noticias_filtradas[\"Link\"]))]\n",
    "\n",
    "filtered_df = filtered_df[list(map(lambda x, y, z: z == \"G1\" or y != \"[]\" or \"santa-catarina\" in x.split(\"https\")[1] or \"/sc/\" in x, filtered_df[\"Link\"], filtered_df[\"Cidade\"], filtered_df[\"Produto\"]))]\n",
    "print(len(filtered_df))\n",
    "outraRegiao = [True]*len(filtered_df)\n",
    "\n",
    "for noticia in range(len(filtered_df)):\n",
    "    text = str(filtered_df.values[noticia][2]) + str(filtered_df.values[noticia][4])\n",
    "    link = filtered_df[\"Link\"][noticia]\n",
    "\n",
    "    for x in RegioesBrasil:\n",
    "        if x in text:\n",
    "            outraRegiao[noticia] = False\n",
    "            break\n",
    "        if len(x) == 2:\n",
    "            if \"F\" + x.lower() + \"%\" in link:\n",
    "                outraRegiao[noticia] = False\n",
    "                print(link)\n",
    "        elif x.lower() in link or x.lower().replace(\" \", \"-\") in link:\n",
    "                outraRegiao[noticia] = False\n",
    "\n",
    "# if filtered_df.values[noticia][]\n",
    "filtered_df = filtered_df[outraRegiao]\n",
    "\n",
    "total_noticias = len(noticias_filtradas)\n",
    "\n",
    "num_noticias_filtradas = len(filtered_df)\n",
    "\n",
    "percentual_filtradas = (num_noticias_filtradas / total_noticias) * 100\n",
    "\n",
    "noticias_filtradas = filtered_df\n",
    "noticias_filtradas.to_excel('./planilhas/Filtradas link SC.xlsx', index=False)\n",
    "\n",
    "print(f\"Total de notícias: {total_noticias}\")\n",
    "print(f\"Número de notícias 'G1' ou 'G1 SC': {num_noticias_filtradas}\")\n",
    "print(f\"Percentual correspondente: {percentual_filtradas:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139e146-7f15-413a-b7e7-0e5a51df00fd",
   "metadata": {},
   "source": [
    "### Mini Relatório: Análise do Código\n",
    "\n",
    "**Objetivo:**  \n",
    "Este bloco realiza a última etapa do processo de filtragem das notícias. Ele utiliza o acesso direto às páginas de cada notícia para buscar informações mais específicas relacionadas a Santa Catarina (SC). Esse passo é necessário devido à inconsistência de padrões nos dados e links, mas é o mais demorado devido ao tempo necessário para carregar cada página individualmente.\n",
    "\n",
    "---\n",
    "\n",
    "**Funcionamento:**  \n",
    "\n",
    "1. **Configuração do Driver Selenium:**  \n",
    "   - Inicializa o driver **Selenium** para automação de navegador, utilizando o Chrome em modo headless (sem interface gráfica).  \n",
    "   - Define configurações para ignorar certificados, notificações e simular um agente de usuário padrão.\n",
    "\n",
    "2. **Leitura dos Dados:**  \n",
    "   - Carrega as notícias filtradas no arquivo `Filtradas link SC.xlsx`.  \n",
    "   - Seleciona apenas as notícias sem cidades previamente definidas, priorizando aquelas que ainda precisam de verificação manual.\n",
    "\n",
    "3. **Iteração com Acesso às Páginas:**  \n",
    "   - Utiliza o Selenium para acessar cada link de notícia.  \n",
    "   - Analisa o conteúdo da página com o **BeautifulSoup**:\n",
    "     - Busca o título (`h1`) e a editora associados à notícia.\n",
    "     - Verifica a presença de \"Santa Catarina\" ou \"santa-catarina\" no título ou editora.\n",
    "\n",
    "4. **Marcação das Notícias Relevantes:**  \n",
    "   - Se a notícia contém referências explícitas a SC no título ou editora, ela é marcada como relevante no vetor `filtro`.\n",
    "\n",
    "5. **Unificação e Exportação dos Resultados:**  \n",
    "   - Combina as notícias recém-confirmadas com aquelas já garantidas como relacionadas a SC.  \n",
    "   - Salva o resultado final no arquivo `Filtragem final.xlsx`.\n",
    "\n",
    "---\n",
    "\n",
    "**Saída:**  \n",
    "- **Notícias confirmadas:** Número de notícias relacionadas a SC encontradas nessa etapa.  \n",
    "- **Arquivo final:** Contém todas as notícias consideradas relevantes para SC após todas as etapas do processo de filtragem.\n",
    "\n",
    "**Nota:**  \n",
    "Essa etapa complementa as anteriores, garantindo maior precisão na classificação, mas tem maior custo computacional devido à necessidade de acessar cada página individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a6bd1-0942-4157-9d8e-7eae5cc1a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import unquote\n",
    "\n",
    "print(\"Inicializando driver...\")\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_argument('--disable-web-security')\n",
    "options.add_argument('--disable-site-isolation-trials')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--allow-running-insecure-content')\n",
    "options.add_argument('--disable-notifications')\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "print(\"Driver iniciado\")\n",
    "\n",
    "verificar_pagina = pd.read_excel(\"./planilhas/Filtradas link SC.xlsx\")\n",
    "verificar_pagina = verificar_pagina[list(map(lambda x: x == \"[]\" , verificar_pagina[\"Cidade\"]))]\n",
    "\n",
    "filtro = [False]*len(verificar_pagina)\n",
    "cont = 0\n",
    "\n",
    "with tqdm(total = len(verificar_pagina), desc = \"Verificando páginas\") as pbar:\n",
    "    for noticia in verificar_pagina[\"Link\"][::-1][1:60]:\n",
    "\n",
    "        # print(noticia)\n",
    "        try:\n",
    "            driver.get(noticia)\n",
    "        except:\n",
    "            print(noticia)\n",
    "            print(\"Error timeout\")\n",
    "            cont+=1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "            \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        title = soup.find(\"h1\", {\"class\": \"content-head__title\"})\n",
    "        editora = soup.find(\"a\", {\"class\": \"header-editoria--link ellip-line\"})\n",
    "        #\n",
    "        \n",
    "        # print(editora)\n",
    "        # textContent = soup.find_all(\"p\", {\"class\": \"content-text__container\"})\n",
    "\n",
    "        \n",
    "        \n",
    "        if not type(title) == str:\n",
    "            cont+=1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "            \n",
    "        if \"santa catarina\" in title.lower() or \"santa catarina\" in editora.lower() or \"santa-catarina\" in editora.lower():\n",
    "            filtro[cont] = True\n",
    "        \n",
    "        cont+=1\n",
    "        pbar.update(1)\n",
    "\n",
    "noticias_encontradas = verificar_pagina[filtro]\n",
    "\n",
    "sc_garantido = pd.read_excel(\"./planilhas/Filtradas link SC.xlsx\")\n",
    "sc_garantido = sc_garantido[list(map(lambda x: x != \"[]\" , sc_garantido[\"Cidade\"]))]\n",
    "\n",
    "\n",
    "pd.concat(noticias_encontradas, sc_garantido)\n",
    "\n",
    "\n",
    "noticias_encontradas.to_excel(\"./planilhas/Filtragem final.xlsx\", index=False)\n",
    "\n",
    "print(f\"Foram achadas {len(noticias_encontradas)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
